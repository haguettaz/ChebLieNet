{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import wandb\n",
    "from gechebnet.data.dataloader import get_train_val_data_loaders\n",
    "from gechebnet.engine.engine import create_supervised_evaluator, create_supervised_trainer\n",
    "from gechebnet.engine.utils import prepare_batch, wandb_log\n",
    "from gechebnet.graph.graph import HyperCubeGraph\n",
    "from gechebnet.model.chebnet import ChebNet\n",
    "from gechebnet.model.optimizer import get_optimizer\n",
    "from gechebnet.utils import random_choice\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Events\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "from torch.nn import NLLLoss\n",
    "from torch.nn.functional import nll_loss\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATASET = \"MNIST\"\n",
    "VAL_RATIO = 0.2\n",
    "NX1, NX2 = (28, 28)\n",
    "\n",
    "IN_CHANNELS = 1\n",
    "OUT_CHANNELS = 10\n",
    "HIDDEN_CHANNELS = 20\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "def get_model(nx3, knn, eps, xi, weight_sigma, weight_kernel, K, pooling):\n",
    "    graphs = [\n",
    "        HyperCubeGraph(\n",
    "            grid_size=(NX1, NX2),\n",
    "            nx3=nx3,\n",
    "            weight_kernel=weight_kernel,\n",
    "            weight_sigma=weight_sigma,\n",
    "            knn=knn,\n",
    "            sigmas=(xi / eps, xi, 1.0),\n",
    "            weight_comp_device=DEVICE,\n",
    "        ),\n",
    "        HyperCubeGraph(\n",
    "            grid_size=(NX1 // 2, NX2 // 2),\n",
    "            nx3=nx3,\n",
    "            weight_kernel=weight_kernel,\n",
    "            weight_sigma=weight_sigma,\n",
    "            knn=knn,\n",
    "            sigmas=(xi / eps, xi, 1.0),\n",
    "            weight_comp_device=DEVICE,\n",
    "        ),\n",
    "        HyperCubeGraph(\n",
    "            grid_size=(NX1 // 2 // 2, NX2 // 2 // 2),\n",
    "            nx3=nx3,\n",
    "            weight_kernel=weight_kernel,\n",
    "            weight_sigma=weight_sigma,\n",
    "            knn=knn,\n",
    "            sigmas=(xi / eps, xi, 1.0),\n",
    "            weight_comp_device=DEVICE,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    model = ChebNet(graphs, K, IN_CHANNELS, OUT_CHANNELS, HIDDEN_CHANNELS, laplacian_device=DEVICE, pooling=pooling)\n",
    "    #while model.capacity < NUM_PARAMS:\n",
    "    #    hidden_channels += 1\n",
    "    #    model = ChebNet(graphs, K, IN_CHANNELS, OUT_CHANNELS, hidden_channels, laplacian_device=DEVICE, pooling=pooling)\n",
    "\n",
    "    print(model.capacity)\n",
    "\n",
    "    return model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "lst = [0, 1, 0, 1]\n",
    "np.std(lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_KNN = 2\n",
    "\n",
    "for knn_exp in [0, 1, 2, 3, 4]:\n",
    "    print(MIN_KNN * 2**knn_exp)\n",
    "    print(MIN_KNN * 2**knn_exp*4)\n",
    "    print(MIN_KNN * 2**knn_exp*16)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "24/(7*7*4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "node_index = torch.arange(10000)\n",
    "node_index.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "xi = 0.01\n",
    "eps = 0.1\n",
    "K = 20\n",
    "knn = 27\n",
    "learning_rate = 1e-3\n",
    "nx3 = 6\n",
    "optimizer = \"adam\"\n",
    "weight_sigma = 1.\n",
    "weight_decay = 0\n",
    "weight_kernel = \"gaussian\"\n",
    "pooling = \"max\"\n",
    "    \n",
    "train_loader, val_loader = get_train_val_data_loaders(DATASET, batch_size=batch_size, val_ratio=VAL_RATIO, data_path=DATA_PATH)\n",
    "\n",
    "model = get_model(nx3, knn, eps, xi, weight_sigma, weight_kernel, K, pooling)\n",
    "\n",
    "optimizer = get_optimizer(model, optimizer, learning_rate, weight_decay)\n",
    "\n",
    "loss_fn = nll_loss\n",
    "metrics = {\"val_mnist_acc\": Accuracy(), \"val_mnist_loss\": Loss(loss_fn)}\n",
    "\n",
    "# create ignite's engines\n",
    "trainer = create_supervised_trainer(\n",
    "    L=nx3, model=model, optimizer=optimizer, loss_fn=loss_fn, device=DEVICE, prepare_batch=prepare_batch\n",
    ")\n",
    "ProgressBar(persist=False, desc=\"Training\").attach(trainer)\n",
    "\n",
    "ProgressBar(persist=False, desc=\"Training\").attach(trainer)\n",
    "\n",
    "evaluator = create_supervised_evaluator(L=nx3, model=model, metrics=metrics, device=DEVICE, prepare_batch=prepare_batch)\n",
    "\n",
    "# track training with wandb\n",
    "_ = trainer.add_event_handler(Events.EPOCH_COMPLETED, wandb_log, evaluator, val_loader)\n",
    "\n",
    "# save best model\n",
    "\n",
    "trainer.run(train_loader, max_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "m = nn.LogSoftmax(dim=1)\n",
    "loss = nn.NLLLoss()\n",
    "# input is of size N x C = 3 x 5\n",
    "input = torch.randn(3, 5, requires_grad=True)\n",
    "# each element in target has to have 0 <= value < C\n",
    "target = torch.tensor([1, 0, 4])\n",
    "output = loss(m(input), target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.rand(10)\n",
    "a.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gechebnet.model.convolution import cheb_conv, ChebConv\n",
    "from gechebnet.model.chebnet import ChebNet\n",
    "\n",
    "from gechebnet.graph.plot import visualize_graph, visualize_neighborhood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChebNet(\n",
    "[HyperCubeGraph(\n",
    "            grid_size=(2, 2),\n",
    "            nx3=2,\n",
    "            weight_kernel=\"gaussian\",\n",
    "            weight_sigma=1.,\n",
    "            knn=2,\n",
    "            sigmas=(xi / eps, xi, 1.0),\n",
    "            weight_comp_device=DEVICE,\n",
    "        )],\n",
    "    10, 1, 10, 20\n",
    ")\n",
    "\n",
    "model.capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = 0.01\n",
    "eps = 0.1\n",
    "\n",
    "graph_1 = HyperCubeGraph(\n",
    "            grid_size=(28, 28),\n",
    "            nx3=6,\n",
    "            weight_kernel=\"gaussian\",\n",
    "            weight_sigma=1.,\n",
    "            knn=26,\n",
    "            sigmas=(xi / eps, xi, 1.0),\n",
    "            weight_comp_device=DEVICE,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_1.num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_2 = HyperCubeGraph(\n",
    "            grid_size=(14, 14),\n",
    "            nx3=6,\n",
    "            weight_kernel=\"gaussian\",\n",
    "            weight_sigma=1.,\n",
    "            knn=26,\n",
    "            sigmas=(xi/ eps, xi, 1.0),  # adapt the metric kernel to the size of the graph\n",
    "            weight_comp_device=DEVICE,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_2.num_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_3 = HyperCubeGraph(\n",
    "            grid_size=(7, 7),\n",
    "            nx3=6,\n",
    "            weight_kernel=\"gaussian\",\n",
    "            weight_sigma=1.,\n",
    "            knn=26,\n",
    "            sigmas=(xi/ eps, xi, 1.0),  # adapt the metric kernel to the size of the graph\n",
    "            weight_comp_device=DEVICE,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_neighborhood(graph_1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_neighborhood(graph_2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = visualize_neighborhood(graph_3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Optional, Tuple, TypeVar\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.transforms import Compose, Normalize, ToTensor, RandomRotation, RandomHorizontalFlip, RandomVerticalFlip\n",
    "\n",
    "MNIST_MEAN, MNIST_STD = (0.1307,), (0.3081,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformation = [RandomRotation(degrees=180), ToTensor(), Normalize(MNIST_MEAN, MNIST_STD)]\n",
    "transformation = [RandomHorizontalFlip(p=0.5), RandomVerticalFlip(p=0.5), ToTensor(), Normalize(MNIST_MEAN, MNIST_STD)]\n",
    "\n",
    "\n",
    "dataset = MNIST(\n",
    "    \"data\", train=False, download=True, transform=Compose(transformation)\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch[0][0].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch[0][1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(batch[0][2].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gechebnet.data.dataloader import get_test_equivariance_data_loader\n",
    "\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1, d2, d3 = get_test_equivariance_data_loader(\"MNIST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(d1))\n",
    "plt.imshow(batch[0][1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(d2))\n",
    "plt.imshow(batch[0][1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(d3))\n",
    "plt.imshow(batch[0][1].squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"batch_size\": {\"distribution\": \"q_log_uniform\", \"min\": math.log(8), \"max\": math.log(256)},\n",
    "\"eps\": {\"distribution\": \"log_uniform\", \"min\": math.log(0.1), \"max\": math.log(1.0)},\n",
    "\"K\": {\"distribution\": \"q_log_uniform\", \"min\": math.log(2), \"max\": math.log(64)},\n",
    "\"knn\": {\"distribution\": \"categorical\", \"values\": [2, 4, 8, 16, 32]},\n",
    "\"learning_rate\": {\"distribution\": \"log_uniform\", \"min\": math.log(1e-5), \"max\": math.log(0.1)},\n",
    "\"nx3\": {\"distribution\": \"int_uniform\", \"min\": 3, \"max\": 9},\n",
    "\"pooling\": {\"distribution\": \"categorical\", \"values\": [\"max\", \"avg\"]},\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# graph\n",
    "nx3 = random.randint(3, 12)\n",
    "eps = math.exp(random.uniform(math.log(0.1), math.log(1.0)))\n",
    "xi = math.exp(random.uniform(math.log(1e-2), math.log(1.0)))\n",
    "knn = random.choice([2, 4, 8, 16, 32])\n",
    "weight_kernel = random.choice([\"cauchy\", \"gaussian\", \"laplacian\"])\n",
    "weight_sigma = math.exp(random.uniform(math.log(0.25), math.log(10)))\n",
    "\n",
    "# network\n",
    "K = random.choice([2, 4, 8, 16, 32, 64])\n",
    "pooling = random.choice([\"max\", \"avg\"])\n",
    "\n",
    "# training\n",
    "batch_size = random.choice([8, 16, 32, 64, 128, 256])\n",
    "learning_rate = math.exp(random.uniform(math.log(1e-5), math.log(0.1)))\n",
    "weight_decay = math.exp(random.uniform(math.log(1e-7), math.log(1e-2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nx3, eps, xi, knn, weight_kernel, weight_sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from gechebnet.graph.graph import HyperCubeGraph\n",
    "\n",
    "NX1 = 28\n",
    "NX2 = 28\n",
    "NX3 = 8\n",
    "\n",
    "POOLING_SIZE = 2\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "import time\n",
    "\n",
    "NUM_ITER = 1\n",
    "\n",
    "\n",
    "def build_graphs(knn):\n",
    "\n",
    "    eps = math.exp(random.uniform(math.log(0.1), math.log(1.0)))\n",
    "    xi = math.exp(random.uniform(math.log(1e-2), math.log(1.0)))\n",
    "\n",
    "    print((xi, xi * eps, 1.0))\n",
    "\n",
    "    times = []\n",
    "    print(f\"KNN = {int(knn * POOLING_SIZE ** 4)} and V = {NX1*NX2*NX3}\")\n",
    "    for _ in range(NUM_ITER):\n",
    "        start = time.time()\n",
    "        graph_1 = HyperCubeGraph(\n",
    "            grid_size=(NX1, NX2),\n",
    "            nx3=NX3,\n",
    "            knn=int(knn * POOLING_SIZE ** 4),\n",
    "            weight_comp_device=DEVICE,\n",
    "            sigmas=(xi, xi * eps, 1.0),\n",
    "        )\n",
    "        end = time.time()\n",
    "        if graph_1.num_nodes > graph_1.num_edges:\n",
    "            print(\"Value Error: an error occured during the construction of the graph\")\n",
    "        times.append(end - start)\n",
    "    print(f\"time: mean {np.mean(times)} std {np.std(times)}\")\n",
    "\n",
    "    times = []\n",
    "    print(f\"KNN = {int(knn * POOLING_SIZE ** 2)} and V = {(NX1//POOLING_SIZE)*(NX2//POOLING_SIZE)*NX3}\")\n",
    "    for _ in range(NUM_ITER):\n",
    "        start = time.time()\n",
    "        graph_2 = HyperCubeGraph(\n",
    "            grid_size=(NX1 // POOLING_SIZE, NX2 // POOLING_SIZE),\n",
    "            nx3=NX3,\n",
    "            knn=int(knn * POOLING_SIZE ** 2),\n",
    "            weight_comp_device=DEVICE,\n",
    "            sigmas=(xi / eps, xi, 1.0),\n",
    "        )\n",
    "        end = time.time()\n",
    "        if graph_2.num_nodes > graph_2.num_edges:\n",
    "            print(\"Value Error: an error occured during the construction of the graph\")\n",
    "        times.append(end - start)\n",
    "    print(f\"time: mean {np.mean(times)} std {np.std(times)}\")\n",
    "\n",
    "    times = []\n",
    "    print(f\"KNN = {int(knn)} and V = {(NX1//POOLING_SIZE//POOLING_SIZE)*(NX2//POOLING_SIZE//POOLING_SIZE)*NX3}\")\n",
    "    for _ in range(NUM_ITER):\n",
    "        start = time.time()\n",
    "        graph_3 = HyperCubeGraph(\n",
    "            grid_size=(NX1 // POOLING_SIZE // POOLING_SIZE, NX2 // POOLING_SIZE // POOLING_SIZE),\n",
    "            nx3=NX3,\n",
    "            knn=int(knn),\n",
    "            weight_comp_device=DEVICE,\n",
    "            sigmas=(xi / eps, xi, 1.0),\n",
    "        )\n",
    "        end = time.time()\n",
    "        if graph_3.num_nodes > graph_3.num_edges:\n",
    "            print(\"Value Error: an error occured during the construction of the graph\")\n",
    "        times.append(end - start)\n",
    "    print(f\"time: mean {np.mean(times)} std {np.std(times)}\")\n",
    "\n",
    "\n",
    "for knn in [2, 4, 8, 16, 32]:\n",
    "    build_graphs(knn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "import random\n",
    "\n",
    "import pykeops\n",
    "import torch\n",
    "import wandb\n",
    "from gechebnet.data.dataloader import get_train_val_data_loaders\n",
    "from gechebnet.engine.engine import create_supervised_evaluator, create_supervised_trainer\n",
    "from gechebnet.engine.utils import prepare_batch, wandb_log\n",
    "from gechebnet.graph.graph import HyperCubeGraph\n",
    "from gechebnet.model.chebnet import ChebNet\n",
    "from gechebnet.model.optimizer import get_optimizer\n",
    "from gechebnet.utils import random_choice\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Events\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "from torch.nn import NLLLoss\n",
    "from torch.nn.functional import nll_loss\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "\n",
    "DATASET_NAME = \"MNIST\"  # STL10\n",
    "VAL_RATIO = 0.2\n",
    "NX1, NX2 = (28, 28)\n",
    "\n",
    "IN_CHANNELS = 1\n",
    "OUT_CHANNELS = 10\n",
    "HIDDEN_CHANNELS = 20\n",
    "POOLING_SIZE = 2\n",
    "\n",
    "EPOCHS = 20\n",
    "OPTIMIZER = \"adam\"\n",
    "\n",
    "NUM_ITER = 10\n",
    "\n",
    "\n",
    "def build_sweep_config():\n",
    "    sweep_config = {\"method\": \"bayes\", \"metric\": {\"name\": \"validation_accuracy\", \"goal\": \"maximize\"}}\n",
    "\n",
    "    sweep_config[\"parameters\"] = {\n",
    "        \"batch_size\": {\"distribution\": \"q_log_uniform\", \"min\": math.log(8), \"max\": math.log(256)},\n",
    "        \"eps\": {\"distribution\": \"log_uniform\", \"min\": math.log(0.1), \"max\": math.log(1.0)},\n",
    "        \"K\": {\"distribution\": \"q_log_uniform\", \"min\": math.log(2), \"max\": math.log(64)},\n",
    "        \"knn\": {\"distribution\": \"categorical\", \"values\": [2, 4, 8, 16, 32]},\n",
    "        \"learning_rate\": {\"distribution\": \"log_uniform\", \"min\": math.log(1e-5), \"max\": math.log(0.1)},\n",
    "        \"nx3\": {\"distribution\": \"int_uniform\", \"min\": 3, \"max\": 9},\n",
    "        \"pooling\": {\"distribution\": \"categorical\", \"values\": [\"max\", \"avg\"]},\n",
    "        \"weight_sigma\": {\"distribution\": \"uniform\", \"min\": 0.25, \"max\": 8.0},\n",
    "        \"weight_decay\": {\"distribution\": \"log_uniform\", \"min\": math.log(1e-6), \"max\": math.log(1e-3)},\n",
    "        \"weight_kernel\": {\"distribution\": \"categorical\", \"values\": [\"cauchy\", \"gaussian\", \"laplacian\"]},\n",
    "        \"xi\": {\"distribution\": \"log_uniform\", \"min\": math.log(1e-2), \"max\": math.log(1.0)},\n",
    "    }\n",
    "\n",
    "    return sweep_config\n",
    "\n",
    "\n",
    "def get_model(nx3, knn, eps, xi, weight_sigma, weight_kernel, K, pooling):\n",
    "\n",
    "    print(\"nx3\", nx3, type(nx3))\n",
    "    print(\"knn\", knn, type(knn))\n",
    "    print(\"eps\", eps, type(eps))\n",
    "    print(\"xi\", xi, type(xi))\n",
    "    print(\"weight_sigma\", weight_sigma, type(weight_sigma))\n",
    "    print(\"weight_kernel\", weight_kernel, type(weight_kernel))\n",
    "    print(\"K\", K, type(K))\n",
    "    print(\"pooling\", pooling, type(pooling))\n",
    "\n",
    "    print(\"NX1, NX2\", NX1, NX2)\n",
    "    print(\"NX1 // POOLING_SIZE, NX2 // POOLING_SIZE\", NX1 // POOLING_SIZE, NX2 // POOLING_SIZE)\n",
    "    print(\n",
    "        \"NX1 // POOLING_SIZE // POOLING_SIZE, NX2 // POOLING_SIZE// POOLING_SIZE\",\n",
    "        NX1 // POOLING_SIZE // POOLING_SIZE,\n",
    "        NX2 // POOLING_SIZE // POOLING_SIZE,\n",
    "    )\n",
    "\n",
    "    # Different graphs are for successive pooling layers\n",
    "\n",
    "    graph_1 = HyperCubeGraph(\n",
    "        grid_size=(NX1, NX2),\n",
    "        nx3=nx3,\n",
    "        knn=int(knn * POOLING_SIZE ** 4),\n",
    "        sigmas=(xi/eps, xi, 1.0),\n",
    "        weight_comp_device=DEVICE,\n",
    "        weight_sigma=weight_sigma,\n",
    "        weight_kernel=weight_kernel\n",
    "    )\n",
    "    if graph_1.num_nodes > graph_1.num_edges:\n",
    "        raise ValueError(f\"An error occured during the computation of the graph\")\n",
    "    wandb.log({f\"graph_1_nodes\": graph_1.num_nodes, f\"graph_1_edges\": graph_1.num_edges})\n",
    "\n",
    "    graph_2 = HyperCubeGraph(\n",
    "        grid_size=(NX1 // POOLING_SIZE, NX2 // POOLING_SIZE),\n",
    "        nx3=nx3,\n",
    "        knn=int(knn * POOLING_SIZE ** 2),\n",
    "        sigmas=(xi/eps, xi, 1.0),\n",
    "        weight_comp_device=DEVICE,\n",
    "        weight_sigma=weight_sigma,\n",
    "        weight_kernel=weight_kernel\n",
    "    )\n",
    "    if graph_2.num_nodes > graph_2.num_edges:\n",
    "        raise ValueError(f\"An error occured during the computation of the graph\")\n",
    "    wandb.log({f\"graph_2_nodes\": graph_2.num_nodes, f\"graph_2_edges\": graph_2.num_edges})\n",
    "\n",
    "    graph_3 = HyperCubeGraph(\n",
    "        grid_size=(NX1 // POOLING_SIZE // POOLING_SIZE, NX2 // POOLING_SIZE // POOLING_SIZE),\n",
    "        nx3=nx3,\n",
    "        knn=int(knn * POOLING_SIZE ** 4),\n",
    "        sigmas=(xi/eps, xi, 1.0),\n",
    "        weight_comp_device=DEVICE,\n",
    "        weight_sigma=weight_sigma,\n",
    "        weight_kernel=weight_kernel\n",
    "    )\n",
    "    if graph_3.num_nodes > graph_3.num_edges:\n",
    "        raise ValueError(f\"An error occured during the computation of the graph\")\n",
    "    wandb.log({f\"graph_3_nodes\": graph_3.num_nodes, f\"graph_3_edges\": graph_3.num_edges})\n",
    "\n",
    "    model = ChebNet(\n",
    "        (graph_1, graph_2, graph_3),\n",
    "        K,\n",
    "        IN_CHANNELS,\n",
    "        OUT_CHANNELS,\n",
    "        HIDDEN_CHANNELS,\n",
    "        laplacian_device=DEVICE,\n",
    "        pooling=pooling,\n",
    "    )\n",
    "\n",
    "    wandb.log({\"capacity\": model.capacity})\n",
    "\n",
    "    return model.to(DEVICE)\n",
    "\n",
    "\n",
    "def train(config=None):\n",
    "    # Initialize a new wandb run\n",
    "    with wandb.init(config=config):\n",
    "        config = wandb.config\n",
    "\n",
    "        # Model and optimizer\n",
    "        model = get_model(\n",
    "            config.nx3,\n",
    "            config.knn,\n",
    "            config.eps,\n",
    "            config.xi,\n",
    "            config.weight_sigma,\n",
    "            config.weight_kernel,\n",
    "            config.K,\n",
    "            config.pooling,\n",
    "        )\n",
    "\n",
    "#         optimizer = get_optimizer(model, OPTIMIZER, config.learning_rate, config.weight_decay)\n",
    "#         loss_fn = nll_loss\n",
    "\n",
    "#         # Trainer and evaluator(s) engines\n",
    "#         trainer = create_supervised_trainer(\n",
    "#             L=config.nx3,\n",
    "#             model=model,\n",
    "#             optimizer=optimizer,\n",
    "#             loss_fn=loss_fn,\n",
    "#             device=DEVICE,\n",
    "#             prepare_batch=prepare_batch,\n",
    "#         )\n",
    "#         ProgressBar(persist=False, desc=\"Training\").attach(trainer)\n",
    "\n",
    "#         metrics = {\"validation_accuracy\": Accuracy(), \"validation_loss\": Loss(loss_fn)}\n",
    "\n",
    "#         evaluator = create_supervised_evaluator(\n",
    "#             L=config.nx3, model=model, metrics=metrics, device=DEVICE, prepare_batch=prepare_batch\n",
    "#         )\n",
    "#         ProgressBar(persist=False, desc=\"Evaluation\").attach(evaluator)\n",
    "\n",
    "#         train_loader, val_loader = get_train_val_data_loaders(\n",
    "#             DATASET_NAME, batch_size=config.batch_size, val_ratio=VAL_RATIO, data_path=DATA_PATH\n",
    "#         )\n",
    "\n",
    "#         # Performance tracking with wandb\n",
    "#         trainer.add_event_handler(Events.EPOCH_COMPLETED, wandb_log, evaluator, val_loader)\n",
    "\n",
    "#         trainer.run(train_loader, max_epochs=EPOCHS)\n",
    "\n",
    "sweep_config = build_sweep_config()\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"gechebnet\")\n",
    "wandb.agent(sweep_id, train, count=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ~/Documents/thesis/GroupEquivariantChebNets/scripts/test_compiled_graphs.py"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
