{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import AvgPool1d, BatchNorm1d, MaxPool1d, Module\n",
    "\n",
    "\n",
    "class ResidualBlock(Module):\n",
    "    def __init__(self, in_channels, out_channels, hidden_channels, K):\n",
    "        super().__init__()\n",
    "        self.conv1 = ChebConv(in_channels, hidden_channels, K)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn2 = BatchNorm1d(hidden_channels)\n",
    "        self.conv2 = ChebConv(in_channels, out_channels, K)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, laplacian):\n",
    "        out = self.conv1(x, laplacian) # (B, C, V)\n",
    "        out = self.relu1(out) # (B, C, V)\n",
    "        out = self.bn2(out) # (B, C, V)\n",
    "        out = self.conv2(out, laplacian) # (B, C, V)\n",
    "        return self.relu2(out + x) # (B, C, V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResGEChebNet(Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        graph: Graph,\n",
    "        K: int,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        hidden_channels: int = 20,\n",
    "        pooling: Optional[str] = \"max\",\n",
    "        device: Optional[Device] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Initialize a ChebNet with convolutional layers and batch normalization.\n",
    "\n",
    "        Args:\n",
    "            graph (Graph): graph.\n",
    "            K (int): degree of the Chebyschev polynomials, the sum goes from indices 0 to K-1.\n",
    "            in_channels (int): number of dimensions of the input layer.\n",
    "            out_channels (int): number of dimensions of the output layer.\n",
    "            hidden_channels (int, optional): number of dimensions of the hidden layers. Defaults to 20.\n",
    "            pooling (str, optional): global pooling function. Defaults to 'max'.\n",
    "            device (Device, optional): computation device. Defaults to None.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: pooling must be 'avg' or 'max'\n",
    "        \"\"\"\n",
    "        super(ResGEChebNet, self).__init__()\n",
    "\n",
    "        self.laplacian = self._normlaplacian(\n",
    "            graph.laplacian(device), lmax=2.0, num_nodes=graph.num_nodes\n",
    "        )\n",
    "\n",
    "        if pooling not in {\"avg\", \"max\"}:\n",
    "            raise ValueError(f\"{pooling} is not a valid value for pooling: must be 'avg' or 'max'\")\n",
    "\n",
    "        self.resblock1 = ResidualBlock(in_channels, hidden_channels, hidden_channels, K)\n",
    "        \n",
    "        self.bn2 = BatchNorm1d(hidden_channels)\n",
    "        self.resblock2 = ResidualBlock(in_channels, hidden_channels, hidden_channels, K)\n",
    "        \n",
    "        self.bn3 = BatchNorm1d(hidden_channels)\n",
    "        self.resblock3 = ResidualBlock(in_channels, hidden_channels, out_channels, K)\n",
    "            \n",
    "        if pooling == \"avg\":\n",
    "            self.pool = AvgPool1d(graph.num_nodes)  # theoretical equivariance\n",
    "        else:\n",
    "            self.pool = MaxPool1d(graph.num_nodes)  # adds some non linearities, better in practice\n",
    "            \n",
    "        self.logsoftmax = Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x: FloatTensor) -> FloatTensor:\n",
    "        \"\"\"\n",
    "        Forward function receiving as input a batch and outputing a prediction on this batch\n",
    "\n",
    "        Args:\n",
    "            x (FloatTensor): the batch to feed the network with.\n",
    "\n",
    "        Returns:\n",
    "            (FloatTensor): the predictions on the batch.\n",
    "        \"\"\"\n",
    "        # Input layer\n",
    "        out = self.resblock1(x, self.laplacian)  # (B, C, V)\n",
    "\n",
    "        # Hidden layers\n",
    "        out = self.bn2(out)\n",
    "        out = self.resblock2(out, self.laplacian)  # (B, C, V)\n",
    "        out = self.bn3(out)\n",
    "        out = self.resblock3(out, self.laplacian)  # (B, C, V)\n",
    "\n",
    "        # Output layer\n",
    "        out = self.pool(out).squeeze()  # (B, C)\n",
    "        return self.logsoftmax(out)  # (B, C)\n",
    "\n",
    "    def _normlaplacian(\n",
    "        self, laplacian: SparseFloatTensor, lmax: float, num_nodes: int\n",
    "    ) -> SparseFloatTensor:\n",
    "        \"\"\"Scale the eigenvalues from [0, lmax] to [-1, 1].\"\"\"\n",
    "        return 2 * laplacian / lmax - sparse_tensor_diag(num_nodes, device=laplacian.device)\n",
    "\n",
    "    @property\n",
    "    def capacity(self) -> int:\n",
    "        \"\"\"\n",
    "        Return the capacity of the network, i.e. its number of trainable parameters.\n",
    "\n",
    "        Returns:\n",
    "            (int): number of trainable parameters of the network.\n",
    "        \"\"\"\n",
    "        return sum(p.numel() for p in self.parameters())\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
