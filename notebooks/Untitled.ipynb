{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from gechebnet.data.dataloader import get_train_val_data_loaders\n",
    "from gechebnet.engine.engine import create_supervised_evaluator, create_supervised_trainer\n",
    "from gechebnet.engine.utils import prepare_batch, wandb_log\n",
    "from gechebnet.graph.graph import HyperCubeGraph\n",
    "from gechebnet.model.chebnet import ChebNet\n",
    "from gechebnet.model.optimizer import get_optimizer\n",
    "from gechebnet.utils import random_choice\n",
    "from ignite.contrib.handlers import ProgressBar\n",
    "from ignite.engine import Events\n",
    "from ignite.metrics import Accuracy, Loss\n",
    "\n",
    "DATA_PATH = \"data\"\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "DATASET = \"MNIST\"\n",
    "VAL_RATIO = 0.2\n",
    "NX1, NX2 = (28, 28)\n",
    "\n",
    "IN_CHANNELS = 1\n",
    "OUT_CHANNELS = 10\n",
    "HIDDEN_CHANNELS = 20\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "\n",
    "def get_model(nx3, knn, eps, xi, weight_sigma, weight_kernel, K, pooling):\n",
    "    graphs = [\n",
    "        HyperCubeGraph(\n",
    "            grid_size=(NX1, NX2),\n",
    "            nx3=nx3,\n",
    "            weight_kernel=weight_kernel,\n",
    "            weight_sigma=weight_sigma,\n",
    "            knn=knn,\n",
    "            sigmas=(xi / eps, xi, 1.0),\n",
    "            weight_comp_device=DEVICE,\n",
    "        ),\n",
    "        HyperCubeGraph(\n",
    "            grid_size=(NX1 // 2, NX2 // 2),\n",
    "            nx3=nx3,\n",
    "            weight_kernel=weight_kernel,\n",
    "            weight_sigma=weight_sigma,\n",
    "            knn=knn,\n",
    "            sigmas=(xi / eps, xi, 1.0),\n",
    "            weight_comp_device=DEVICE,\n",
    "        ),\n",
    "        HyperCubeGraph(\n",
    "            grid_size=(NX1 // 2 // 2, NX2 // 2 // 2),\n",
    "            nx3=nx3,\n",
    "            weight_kernel=weight_kernel,\n",
    "            weight_sigma=weight_sigma,\n",
    "            knn=knn,\n",
    "            sigmas=(xi / eps, xi, 1.0),\n",
    "            weight_comp_device=DEVICE,\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    model = ChebNet(graphs, K, IN_CHANNELS, OUT_CHANNELS, HIDDEN_CHANNELS, laplacian_device=DEVICE, pooling=pooling)\n",
    "    #while model.capacity < NUM_PARAMS:\n",
    "    #    hidden_channels += 1\n",
    "    #    model = ChebNet(graphs, K, IN_CHANNELS, OUT_CHANNELS, hidden_channels, laplacian_device=DEVICE, pooling=pooling)\n",
    "\n",
    "    print(model.capacity)\n",
    "\n",
    "    return model.to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "xi = 0.01\n",
    "eps = 0.1\n",
    "K = 20\n",
    "knn = 8\n",
    "learning_rate = 1e-3\n",
    "nx3 = 1\n",
    "optimizer = \"adam\"\n",
    "weight_sigma = 1.\n",
    "weight_decay = 1e-6\n",
    "weight_kernel = \"gaussian\"\n",
    "pooling = \"max\"\n",
    "    \n",
    "train_loader, val_loader = get_train_val_data_loaders(DATASET, batch_size=batch_size, val_ratio=VAL_RATIO, data_path=DATA_PATH)\n",
    "\n",
    "model = get_model(nx3, knn, eps, xi, weight_sigma, weight_kernel, K, pooling)\n",
    "\n",
    "optimizer = get_optimizer(model, optimizer, learning_rate, weight_decay)\n",
    "\n",
    "loss_fn = F.nll_loss\n",
    "metrics = {\"val_mnist_acc\": Accuracy(), \"val_mnist_loss\": Loss(loss_fn)}\n",
    "\n",
    "# create ignite's engines\n",
    "trainer = create_supervised_trainer(\n",
    "    nx=(NX1, NX2, nx3), model=model, optimizer=optimizer, loss_fn=F.nll_loss, device=DEVICE, prepare_batch=prepare_batch\n",
    ")\n",
    "\n",
    "evaluator = create_supervised_evaluator(nx=(NX1, NX2, nx3), model=model, metrics=metrics, device=DEVICE, prepare_batch=prepare_batch)\n",
    "\n",
    "# track training with wandb\n",
    "_ = trainer.add_event_handler(Events.EPOCH_COMPLETED, wandb_log, evaluator, val_loader)\n",
    "\n",
    "# save best model\n",
    "#trainer.run(train_loader, max_epochs=EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gechebnet.model.convolution import cheb_conv, ChebConv\n",
    "from gechebnet.model.chebnet import ChebNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ChebNet(\n",
    "[HyperCubeGraph(\n",
    "            grid_size=(2, 2),\n",
    "            nx3=2,\n",
    "            weight_kernel=\"gaussian\",\n",
    "            weight_sigma=1.,\n",
    "            knn=2,\n",
    "            sigmas=(xi / eps, xi, 1.0),\n",
    "            weight_comp_device=DEVICE,\n",
    "        )],\n",
    "    10, 1, 10, 20\n",
    ")\n",
    "\n",
    "model.capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xi = 0.01\n",
    "eps = 0.1\n",
    "\n",
    "graph = HyperCubeGraph(\n",
    "            grid_size=(2, 2),\n",
    "            nx3=2,\n",
    "            weight_kernel=\"gaussian\",\n",
    "            weight_sigma=1.,\n",
    "            knn=2,\n",
    "            sigmas=(xi / eps, xi, 1.0),\n",
    "            weight_comp_device=DEVICE,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph.laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1, 2, 8)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = torch.rand(2, 2, 3)\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cheb_conv(graph.laplacian, x, w).permute(1, 2, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.exp(-1/(2*(0.1)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = ChebConv(graph, 2, 3, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.laplacian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.ones(1, 2, 8)\n",
    "C(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.rand(1, 1, 2, 2)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "B, C, H, W = x.shape  # (B, C, H, W)\n",
    "\n",
    "x = x.unsqueeze(2).expand(B, C, 2, 2, 2).reshape(B, C, -1)  # (B, C, L*H*W)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.reshape(B, C, -1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_operator(x, mod, offset):\n",
    "    return x - mod * ((x-offset)//mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "for x in np.linspace(-math.pi, math.pi, 100):\n",
    "    if mod_operator(x, math.pi, -math.pi/2) != modulo_operator(x, math.pi, -math.pi/2):\n",
    "        print(x, mod_operator(x, math.pi, -math.pi/2), modulo_operator(x, math.pi, -math.pi/2) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def modulo_operator(x, mod, offset):\n",
    "    if offset <= x < offset + mod:\n",
    "        return x + 0.5*mod + offset\n",
    "    \n",
    "    if offset - mod <= x < offset:\n",
    "        return x + 1.5*mod + offset\n",
    "    \n",
    "    if offset + mod <= x < offset + 2*mod:\n",
    "        return x - 0.5*mod + offset\n",
    "    \n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modulo_operator(0, math.pi, -math.pi/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mod_operator(x, mod, offset):\n",
    "    \n",
    "    range1 = LazyTensor.step(-(x).abs() + math.pi / 2)  # - pi/2 <= y3 <= pi/2\n",
    "    range2 = LazyTensor.step(xi[2] - xj[2] - math.pi / 2 - eps)  # y3 + eps <= x3 - pi/2\n",
    "    range3 = LazyTensor.step(xj[2] - xi[2] - math.pi / 2 - eps)  # y3 - eps >= x3 + pi/2\n",
    "    \n",
    "    dx3 = range1 * (x) + range2 * (xj[2] - xi[2] + math.pi) + range3 * (xj[2] - xi[2] - math.pi)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
